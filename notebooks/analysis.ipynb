{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Robustness of AI Text Detectors\n",
    "\n",
    "**Research Question:** How fragile are AI text detectors (GPTZero) to prompt engineering, and what linguistic features most influence detection?\n",
    "\n",
    "This notebook analyzes the results of a systematic ablation study testing prompt engineering strategies against GPTZero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path so we can import src modules\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as sp_stats\n",
    "\n",
    "from src import config\n",
    "from src.analysis import (\n",
    "    load_results,\n",
    "    summarize_variants,\n",
    "    summarize_by_topic,\n",
    "    compare_to_baseline,\n",
    "    compare_all_dimensions,\n",
    "    rank_variants,\n",
    "    temperature_summary,\n",
    "    human_baseline_summary,\n",
    "    export_summary_csv,\n",
    ")\n",
    "\n",
    "# Plot styling\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 60)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "\n",
    "THRESHOLD = config.DETECTION_PASS_THRESHOLD\n",
    "print(f\"Detection pass threshold: completely_generated_prob < {THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw results\n",
    "df = load_results()\n",
    "print(f\"Loaded {len(df)} records\")\n",
    "print(f\"Phases: {df['phase'].value_counts().to_dict()}\")\n",
    "print(f\"Dimensions: {df['dimension'].unique().tolist()}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Human Baselines\n",
    "\n",
    "GPTZero scores on real student essays establish the false positive rate.\n",
    "If GPTZero flags a significant fraction of human essays, then \"passing\" detection is less meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_summary = human_baseline_summary(df)\n",
    "if human_summary.empty:\n",
    "    print(\"No human baseline data found. Add .txt files to data/human_baselines/ and run the experiment.\")\n",
    "else:\n",
    "    display(human_summary)\n",
    "    \n",
    "    human_df = df[df[\"phase\"] == \"human_baseline\"]\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.bar(range(len(human_df)), human_df[\"overall_ai_prob\"], color=\"steelblue\")\n",
    "    ax.axhline(y=THRESHOLD, color=\"red\", linestyle=\"--\", label=f\"Threshold ({THRESHOLD})\")\n",
    "    ax.set_xlabel(\"Essay\")\n",
    "    ax.set_ylabel(\"AI Probability\")\n",
    "    ax.set_title(\"GPTZero Scores on Human-Written Essays\")\n",
    "    ax.set_xticks(range(len(human_df)))\n",
    "    ax.set_xticklabels(human_df[\"topic\"].values, rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature Sweep (Tier 6)\n",
    "\n",
    "Detection probability vs. generation temperature.\n",
    "Higher temperature = more randomness = potentially harder to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temperature_summary(df)\n",
    "if temp_df.empty:\n",
    "    print(\"No gen_params_sweep data found. Run the temperature sweep first.\")\n",
    "else:\n",
    "    display(temp_df[[\"variant_label\", \"temperature\", \"n\", \"mean_ai_prob\", \"std_ai_prob\", \"pass_rate\"]])\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: mean AI prob by temperature\n",
    "    axes[0].errorbar(\n",
    "        temp_df[\"temperature\"], temp_df[\"mean_ai_prob\"],\n",
    "        yerr=temp_df[\"std_ai_prob\"], fmt=\"o-\", capsize=5, color=\"coral\"\n",
    "    )\n",
    "    axes[0].axhline(y=THRESHOLD, color=\"red\", linestyle=\"--\", alpha=0.5, label=f\"Threshold ({THRESHOLD})\")\n",
    "    axes[0].set_xlabel(\"Temperature\")\n",
    "    axes[0].set_ylabel(\"Mean AI Probability\")\n",
    "    axes[0].set_title(\"Detection Probability vs Temperature\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Right: pass rate by temperature\n",
    "    axes[1].bar(temp_df[\"temperature\"].astype(str), temp_df[\"pass_rate\"] * 100, color=\"seagreen\")\n",
    "    axes[1].set_xlabel(\"Temperature\")\n",
    "    axes[1].set_ylabel(\"Pass Rate (%)\")\n",
    "    axes[1].set_title(\"Detection Pass Rate vs Temperature\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ablation Results\n",
    "\n",
    "For each prompt dimension (Tiers 1-5), compare all variants to the baseline.\n",
    "Box plots show the distribution of detection scores, and we report Cohen's d effect sizes and Mann-Whitney U p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_df = df[df[\"phase\"] == \"ablation\"]\n",
    "if ablation_df.empty:\n",
    "    print(\"No ablation data found. Run the ablation experiments first.\")\n",
    "else:\n",
    "    dimensions = ablation_df[\"dimension\"].unique()\n",
    "    n_dims = len(dimensions)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_dims, 1, figsize=(12, 5 * n_dims))\n",
    "    if n_dims == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, dim in zip(axes, dimensions):\n",
    "        dim_data = ablation_df[ablation_df[\"dimension\"] == dim]\n",
    "        order = dim_data.groupby(\"variant_label\")[\"overall_ai_prob\"].mean().sort_values().index\n",
    "        \n",
    "        sns.boxplot(data=dim_data, x=\"variant_label\", y=\"overall_ai_prob\", order=order, ax=ax)\n",
    "        ax.axhline(y=THRESHOLD, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "        ax.set_title(f\"Tier: {dim.title()} — Detection Score Distribution\")\n",
    "        ax.set_xlabel(\"Variant\")\n",
    "        ax.set_ylabel(\"AI Probability\")\n",
    "        ax.tick_params(axis=\"x\", rotation=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparisons: each variant vs its dimension's baseline\n",
    "stats_df = compare_all_dimensions(df)\n",
    "if stats_df.empty:\n",
    "    print(\"No ablation data to compare.\")\n",
    "else:\n",
    "    display(\n",
    "        stats_df[\n",
    "            [\"dimension\", \"variant_id\", \"variant_label\", \"baseline_mean\",\n",
    "             \"variant_mean\", \"cohens_d\", \"p_value\", \"n_variant\"]\n",
    "        ].sort_values(\"cohens_d\", ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect size visualization\n",
    "if not stats_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, len(stats_df) * 0.5)))\n",
    "    sorted_stats = stats_df.sort_values(\"cohens_d\")\n",
    "    colors = [\"green\" if d > 0 else \"red\" for d in sorted_stats[\"cohens_d\"]]\n",
    "    \n",
    "    ax.barh(\n",
    "        sorted_stats[\"variant_label\"] + \" (\" + sorted_stats[\"dimension\"] + \")\",\n",
    "        sorted_stats[\"cohens_d\"],\n",
    "        color=colors, alpha=0.7\n",
    "    )\n",
    "    ax.axvline(x=0, color=\"black\", linewidth=0.5)\n",
    "    ax.axvline(x=0.2, color=\"gray\", linestyle=\":\", alpha=0.5, label=\"Small (0.2)\")\n",
    "    ax.axvline(x=0.5, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Medium (0.5)\")\n",
    "    ax.axvline(x=0.8, color=\"gray\", linestyle=\"-\", alpha=0.5, label=\"Large (0.8)\")\n",
    "    ax.set_xlabel(\"Cohen's d (positive = reduced detection vs baseline)\")\n",
    "    ax.set_title(\"Effect Sizes: Prompt Variants vs Baselines\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction Effects\n",
    "\n",
    "Do persona + linguistic texture combine additively, or is there synergy or interference?\n",
    "Compare composite prompt scores to the sum of individual dimension effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_df = df[df[\"phase\"] == \"composite\"]\n",
    "if composite_df.empty:\n",
    "    print(\"No composite data yet. Run composite experiments after ablation analysis.\")\n",
    "    print(\"\\nTo check for interactions, define composite_prompts in taxonomy.yaml\")\n",
    "    print(\"combining the top performers from each tier.\")\n",
    "else:\n",
    "    # Compare composite performance to individual component performance\n",
    "    summary = summarize_variants(df)\n",
    "    composite_summary = summary[summary[\"phase\"] == \"composite\"]\n",
    "    ablation_summary = summary[summary[\"phase\"] == \"ablation\"]\n",
    "    \n",
    "    print(\"Composite prompt performance:\")\n",
    "    display(composite_summary[[\"variant_label\", \"n\", \"mean_ai_prob\", \"std_ai_prob\", \"pass_rate\"]])\n",
    "    \n",
    "    print(\"\\nBest individual variants (for comparison):\")\n",
    "    best_individual = ablation_summary.nsmallest(5, \"mean_ai_prob\")\n",
    "    display(best_individual[[\"dimension\", \"variant_label\", \"mean_ai_prob\", \"pass_rate\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Composite Prompt Performance\n",
    "\n",
    "Final scores for composite prompts (combined best-of-each-tier) with confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if composite_df.empty:\n",
    "    print(\"No composite data yet.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Left: box plot of composite scores\n",
    "    sns.boxplot(data=composite_df, x=\"variant_label\", y=\"overall_ai_prob\", ax=axes[0])\n",
    "    axes[0].axhline(y=THRESHOLD, color=\"red\", linestyle=\"--\", alpha=0.5, label=f\"Threshold ({THRESHOLD})\")\n",
    "    axes[0].set_title(\"Composite Prompt Detection Scores\")\n",
    "    axes[0].set_ylabel(\"AI Probability\")\n",
    "    axes[0].legend()\n",
    "    axes[0].tick_params(axis=\"x\", rotation=20)\n",
    "    \n",
    "    # Right: pass rate comparison across topics\n",
    "    topic_summary = summarize_by_topic(composite_df)\n",
    "    pivot = topic_summary.pivot(index=\"topic\", columns=\"variant_label\", values=\"pass_rate\")\n",
    "    pivot.plot(kind=\"bar\", ax=axes[1])\n",
    "    axes[1].set_title(\"Pass Rate by Topic\")\n",
    "    axes[1].set_ylabel(\"Pass Rate\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[1].legend(title=\"Composite\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Detector Validation\n",
    "\n",
    "Do results hold on detectors other than GPTZero?\n",
    "If multiple detectors were used, compare scores here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = df[\"detector\"].unique()\n",
    "if len(detectors) <= 1:\n",
    "    print(f\"Only one detector used: {detectors[0]}\")\n",
    "    print(\"To validate findings, run the winning prompts through a second detector\")\n",
    "    print(\"(ZeroGPT, Originality.ai, or Sapling).\")\n",
    "else:\n",
    "    print(f\"Detectors: {detectors.tolist()}\")\n",
    "    for det in detectors:\n",
    "        det_data = df[df[\"detector\"] == det]\n",
    "        summary = summarize_variants(det_data)\n",
    "        print(f\"\\n--- {det} ---\")\n",
    "        display(summary[[\"variant_id\", \"variant_label\", \"mean_ai_prob\", \"pass_rate\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Essay Comparison\n",
    "\n",
    "Side-by-side examples showing per-sentence detection highlighting.\n",
    "Which sentences get flagged and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_essay_comparison(df, variant_a_id, variant_b_id, topic_substr=None):\n",
    "    \"\"\"Display two essays side by side with their detection scores.\"\"\"\n",
    "    for vid, label in [(variant_a_id, \"A\"), (variant_b_id, \"B\")]:\n",
    "        subset = df[df[\"variant_id\"] == vid]\n",
    "        if topic_substr:\n",
    "            subset = subset[subset[\"topic\"].str.contains(topic_substr, case=False)]\n",
    "        if subset.empty:\n",
    "            print(f\"No data for variant {vid}\")\n",
    "            continue\n",
    "        row = subset.iloc[0]\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Essay {label}: {row['variant_label']} ({row['variant_id']})\")\n",
    "        print(f\"AI Prob: {row['overall_ai_prob']:.3f} | Burstiness: {row.get('burstiness', 'N/A')}\")\n",
    "        print(f\"Flagged sentences: {row['flagged_sentence_pct']:.1f}%\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(row[\"essay_text\"][:1500])\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Example: compare baseline vs best-performing variant\n",
    "if not ablation_df.empty:\n",
    "    ranked = rank_variants(df)\n",
    "    if not ranked.empty:\n",
    "        best_vid = ranked.iloc[0][\"variant_id\"]\n",
    "        # Find the baseline for the best variant's dimension\n",
    "        best_dim = ablation_df[ablation_df[\"variant_id\"] == best_vid][\"dimension\"].iloc[0]\n",
    "        baseline_vid = [v for v in ablation_df[ablation_df[\"dimension\"] == best_dim][\"variant_id\"].unique() if v.endswith(\"a\")][0]\n",
    "        print(f\"Comparing baseline ({baseline_vid}) vs best variant ({best_vid}):\\n\")\n",
    "        show_essay_comparison(df, baseline_vid, best_vid)\n",
    "else:\n",
    "    print(\"No ablation data available for essay comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Findings Summary\n",
    "\n",
    "Key takeaways from the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINDINGS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall variant ranking\n",
    "ranked = rank_variants(df)\n",
    "if not ranked.empty:\n",
    "    print(\"\\nTop 5 most effective variants (lowest detection):\")\n",
    "    for i, row in ranked.head(5).iterrows():\n",
    "        print(f\"  {i+1}. {row['variant_label']} ({row['variant_id']}) — \"\n",
    "              f\"mean AI prob: {row['mean_ai_prob']:.3f}, pass rate: {row['pass_rate']:.0%}\")\n",
    "\n",
    "# Dimension ranking by effect size\n",
    "all_stats = compare_all_dimensions(df)\n",
    "if not all_stats.empty:\n",
    "    print(\"\\nDimension impact ranking (by max Cohen's d):\")\n",
    "    dim_max = all_stats.groupby(\"dimension\")[\"cohens_d\"].max().sort_values(ascending=False)\n",
    "    for dim, d in dim_max.items():\n",
    "        print(f\"  {dim}: max Cohen's d = {d:.3f}\")\n",
    "\n",
    "# Human baseline context\n",
    "hs = human_baseline_summary(df)\n",
    "if not hs.empty:\n",
    "    fpr = hs.iloc[0][\"false_positive_rate\"]\n",
    "    print(f\"\\nHuman baseline false positive rate: {fpr:.0%}\")\n",
    "    print(f\"  (GPTZero flags {fpr:.0%} of genuine human essays as AI-generated)\")\n",
    "\n",
    "# Temperature finding\n",
    "temp = temperature_summary(df)\n",
    "if not temp.empty:\n",
    "    best_temp = temp.loc[temp[\"mean_ai_prob\"].idxmin()]\n",
    "    print(f\"\\nOptimal temperature: {best_temp['temperature']} \"\n",
    "          f\"(mean AI prob: {best_temp['mean_ai_prob']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary CSV for reference\n",
    "if len(df) > 0:\n",
    "    out_path = export_summary_csv(df)\n",
    "    print(f\"Summary exported to {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
